{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "from dataset import *\n",
    "from model import *\n",
    "from metrics import *\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "from carbontracker.tracker import CarbonTracker  # Monitoring carbon footprint\n",
    "from carbontracker import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args):\n",
    "    dataset = args['dataset']\n",
    "    img_path = dataset['img_path']\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    # Load input and reference data from .npy files :\n",
    "    train_data = Interpolated_Img_Dataset(img_path, 'normalized_input_physical_data_1998_2015.npy', 'ln_Chl_ref_norm_1998_2015.npy', \n",
    "                                        transform=transform, normalize=False) \n",
    "\n",
    "    return train_data\n",
    "\n",
    "def get_loader(train_data, args):\n",
    "\n",
    "    loader = args['dataloader']\n",
    "    # Define train, validation and test datasets\n",
    "    # train:2003-2010, val:1998-2001, test:2012-2015\n",
    "    train_idx, val_idx, test_idx = [i for i in range(60,156)], [i for i in range(0,48)], [i for i in range(168,216)] \n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx) \n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # convert to data loaders :\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=loader['train_bs'],\n",
    "        sampler=train_sampler, num_workers=loader['num_workers'])\n",
    "    val_loader = torch.utils.data.DataLoader(train_data, batch_size=loader['val_bs'], \n",
    "        sampler=val_sampler, num_workers=loader['num_workers'])\n",
    "    test_loader = torch.utils.data.DataLoader(train_data, batch_size=loader['test_bs'], \n",
    "        sampler=test_sampler, num_workers=loader['num_workers'])\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader):  48\n",
      "len(validate_loader) 24\n",
      "len(test_loader) 24\n",
      "input data and target:\n",
      "torch.Size([2, 9, 178, 358]) torch.Size([2, 1, 178, 358])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config_file = './config/Chl_CNN.yaml'\n",
    "with open(config_file, 'r') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "train_data = get_dataset(args)\n",
    "train_loader, val_loader, test_loader = get_loader(train_data, args)\n",
    "\n",
    "print('len(train_loader): ', len(train_loader))\n",
    "print('len(validate_loader)', len(val_loader))\n",
    "print('len(test_loader)', len(test_loader))\n",
    "\n",
    "for data, target in train_loader:\n",
    "    print('input data and target:')\n",
    "    print(data.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args['device'] if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = args['epochs']\n",
    "\n",
    "opt = args['optimizer']['name']\n",
    "lr = args['optimizer']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of 8 sub-models Mi with the architecture :\n",
      "CNN_M1(\n",
      "  (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.35, inplace=False)\n",
      ")\n",
      "and one attention module W that outputs 8 weighted maps :\n",
      "CNN_W(\n",
      "  (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.35, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "model_M1 = CNN_M1()\n",
    "model_M2 = CNN_M2()\n",
    "model_M3 = CNN_M3()\n",
    "model_M4 = CNN_M4()\n",
    "model_M5 = CNN_M5()\n",
    "model_M6 = CNN_M6()\n",
    "model_M7 = CNN_M7()\n",
    "model_M8 = CNN_M8()\n",
    "model_W = CNN_W()\n",
    "\n",
    "# 将其参数和输入数据转换为双精度浮点数\n",
    "model_M1.to(device)  \n",
    "model_M2.to(device)  \n",
    "model_M3.to(device)  \n",
    "model_M4.to(device)   \n",
    "model_M5.to(device)    \n",
    "model_M6.to(device)\n",
    "model_M7.to(device)   \n",
    "model_M8.to(device) \n",
    "model_W.to(device) \n",
    "\n",
    "model_1 = model_M1.double()     \n",
    "model_2 = model_M2.double()\n",
    "model_3 = model_M3.double()\n",
    "model_4 = model_M4.double()\n",
    "model_5 = model_M5.double()   \n",
    "model_6 = model_M6.double()\n",
    "model_7 = model_M7.double()\n",
    "model_8 = model_M8.double()\n",
    "model_W = model_W.double()\n",
    "\n",
    "# specify loss function \n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer_1 = getattr(optim, opt)(model_1.parameters(), lr=lr)\n",
    "optimizer_2 = getattr(optim, opt)(model_2.parameters(), lr=lr)\n",
    "optimizer_3 = getattr(optim, opt)(model_3.parameters(), lr=lr)\n",
    "optimizer_4 = getattr(optim, opt)(model_4.parameters(), lr=lr)\n",
    "optimizer_5 = getattr(optim, opt)(model_5.parameters(), lr=lr)\n",
    "optimizer_6 = getattr(optim, opt)(model_6.parameters(), lr=lr)\n",
    "optimizer_7 = getattr(optim, opt)(model_7.parameters(), lr=lr)\n",
    "optimizer_8 = getattr(optim, opt)(model_8.parameters(), lr=lr)\n",
    "optimizer_W = getattr(optim, opt)(model_W.parameters(), lr=lr)\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.MultiStepLR(optimizer_1, milestones=[400], gamma=0.1)\n",
    "scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer_2, milestones=[400], gamma=0.1)\n",
    "scheduler3 = torch.optim.lr_scheduler.MultiStepLR(optimizer_3, milestones=[400], gamma=0.1)\n",
    "scheduler4 = torch.optim.lr_scheduler.MultiStepLR(optimizer_4, milestones=[400], gamma=0.1)\n",
    "scheduler5 = torch.optim.lr_scheduler.MultiStepLR(optimizer_5, milestones=[400], gamma=0.1)\n",
    "scheduler6 = torch.optim.lr_scheduler.MultiStepLR(optimizer_6, milestones=[400], gamma=0.1)\n",
    "scheduler7 = torch.optim.lr_scheduler.MultiStepLR(optimizer_7, milestones=[400], gamma=0.1)\n",
    "scheduler8 = torch.optim.lr_scheduler.MultiStepLR(optimizer_8, milestones=[400], gamma=0.1)\n",
    "schedulerW = torch.optim.lr_scheduler.MultiStepLR(optimizer_W, milestones=[400], gamma=0.1)\n",
    "\n",
    "print('Definition of 8 sub-models Mi with the architecture :') \n",
    "print(model_1)\n",
    "print('and one attention module W that outputs 8 weighted maps :')\n",
    "print(model_W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify a folder name in which the trained odels will be saved, as well as their root name :\n",
    "folder_name = './checkpoint/'\n",
    "model_name = 'MMCNN'\n",
    "\n",
    "folder_name2 = folder_name + model_name + '/'\n",
    "model_save_1 = folder_name + model_name + '_1.pt'\n",
    "model_save_2 = folder_name + model_name + '_2.pt'\n",
    "model_save_3 = folder_name + model_name + '_3.pt'\n",
    "model_save_4 = folder_name + model_name + '_4.pt'\n",
    "model_save_5 = folder_name + model_name + '_5.pt'\n",
    "model_save_6 = folder_name + model_name + '_6.pt'\n",
    "model_save_7 = folder_name + model_name + '_7.pt'\n",
    "model_save_8 = folder_name + model_name + '_8.pt'\n",
    "model_save_W = folder_name + model_name + '_W.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Monitoring metrics during training loop :\n",
    "loss_mask = 0\n",
    "loss_fig = 10\n",
    "loss_fig_epoque_affichage = 10  # loss绘图间隔的epoch\n",
    "\n",
    "# Import continental mask to compute the loss only on the ocean (not on the land)\n",
    "img_path = args['dataset']['img_path']\n",
    "mask = torch.load(os.path.join(img_path, 'inter_mask.pt'))\n",
    "mask = mask.bool()\n",
    "mask = mask.reshape([1,1,178,358])\n",
    "mask.to(device)\n",
    "\n",
    "batch_size = args['dataloader']['train_bs']\n",
    "\n",
    "\n",
    "# Check for time computation\n",
    "tps_ini = time.process_time() \n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# device = torch.device(\"cuda:1\")s\n",
    "\n",
    "# Create output folder to save the data\n",
    "if not os.path.exists(folder_name2):\n",
    "    os.makedirs(folder_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 \tTraining Loss: 0.157509 \tValidation Loss 1: 0.129430 \ttime: 8min\t\n",
      "Epoch: 2/50 \tTraining Loss: 0.137176 \tValidation Loss 1: 0.117465 \ttime: 12min\t\n",
      "Epoch: 3/50 \tTraining Loss: 0.122529 \tValidation Loss 1: 0.112572 \ttime: 17min\t\n",
      "Epoch: 4/50 \tTraining Loss: 0.113231 \tValidation Loss 1: 0.110051 \ttime: 21min\t\n",
      "Epoch: 5/50 \tTraining Loss: 0.108636 \tValidation Loss 1: 0.101121 \ttime: 26min\t\n",
      "Epoch: 6/50 \tTraining Loss: 0.106177 \tValidation Loss 1: 0.086066 \ttime: 31min\t\n",
      "Epoch: 7/50 \tTraining Loss: 0.097881 \tValidation Loss 1: 0.086089 \ttime: 36min\t\n",
      "Epoch: 8/50 \tTraining Loss: 0.098116 \tValidation Loss 1: 0.088728 \ttime: 40min\t\n",
      "Epoch: 9/50 \tTraining Loss: 0.090387 \tValidation Loss 1: 0.083048 \ttime: 44min\t\n",
      "Epoch: 10/50 \tTraining Loss: 0.089115 \tValidation Loss 1: 0.081574 \ttime: 49min\t\n",
      "Epoch: 11/50 \tTraining Loss: 0.085814 \tValidation Loss 1: 0.074712 \ttime: 53min\t\n",
      "Epoch: 12/50 \tTraining Loss: 0.084457 \tValidation Loss 1: 0.072972 \ttime: 58min\t\n",
      "Epoch: 13/50 \tTraining Loss: 0.082827 \tValidation Loss 1: 0.076346 \ttime: 62min\t\n",
      "Epoch: 14/50 \tTraining Loss: 0.081876 \tValidation Loss 1: 0.071673 \ttime: 66min\t\n",
      "Epoch: 15/50 \tTraining Loss: 0.077846 \tValidation Loss 1: 0.077187 \ttime: 71min\t\n",
      "Epoch: 16/50 \tTraining Loss: 0.076314 \tValidation Loss 1: 0.069288 \ttime: 75min\t\n",
      "Epoch: 17/50 \tTraining Loss: 0.075311 \tValidation Loss 1: 0.068870 \ttime: 80min\t\n",
      "Epoch: 18/50 \tTraining Loss: 0.075334 \tValidation Loss 1: 0.077702 \ttime: 84min\t\n",
      "Epoch: 19/50 \tTraining Loss: 0.073407 \tValidation Loss 1: 0.068154 \ttime: 89min\t\n",
      "Epoch: 20/50 \tTraining Loss: 0.071865 \tValidation Loss 1: 0.068979 \ttime: 93min\t\n",
      "Epoch: 21/50 \tTraining Loss: 0.070378 \tValidation Loss 1: 0.065089 \ttime: 98min\t\n",
      "Epoch: 22/50 \tTraining Loss: 0.070135 \tValidation Loss 1: 0.064228 \ttime: 102min\t\n",
      "Epoch: 23/50 \tTraining Loss: 0.070062 \tValidation Loss 1: 0.065955 \ttime: 107min\t\n",
      "Epoch: 24/50 \tTraining Loss: 0.067548 \tValidation Loss 1: 0.064390 \ttime: 111min\t\n",
      "Epoch: 25/50 \tTraining Loss: 0.067655 \tValidation Loss 1: 0.064951 \ttime: 116min\t\n",
      "Epoch: 26/50 \tTraining Loss: 0.065923 \tValidation Loss 1: 0.062431 \ttime: 120min\t\n",
      "Epoch: 27/50 \tTraining Loss: 0.064766 \tValidation Loss 1: 0.061532 \ttime: 125min\t\n",
      "Epoch: 28/50 \tTraining Loss: 0.064955 \tValidation Loss 1: 0.058633 \ttime: 129min\t\n",
      "Epoch: 29/50 \tTraining Loss: 0.063902 \tValidation Loss 1: 0.067313 \ttime: 134min\t\n",
      "Epoch: 30/50 \tTraining Loss: 0.063523 \tValidation Loss 1: 0.057960 \ttime: 138min\t\n",
      "Epoch: 31/50 \tTraining Loss: 0.061941 \tValidation Loss 1: 0.061367 \ttime: 142min\t\n",
      "Epoch: 32/50 \tTraining Loss: 0.061603 \tValidation Loss 1: 0.059846 \ttime: 147min\t\n",
      "Epoch: 33/50 \tTraining Loss: 0.061451 \tValidation Loss 1: 0.058164 \ttime: 151min\t\n",
      "Epoch: 34/50 \tTraining Loss: 0.060711 \tValidation Loss 1: 0.056766 \ttime: 155min\t\n",
      "Epoch: 35/50 \tTraining Loss: 0.062135 \tValidation Loss 1: 0.057722 \ttime: 160min\t\n",
      "Epoch: 36/50 \tTraining Loss: 0.060105 \tValidation Loss 1: 0.058535 \ttime: 164min\t\n",
      "Epoch: 37/50 \tTraining Loss: 0.059267 \tValidation Loss 1: 0.053854 \ttime: 168min\t\n",
      "Epoch: 38/50 \tTraining Loss: 0.058893 \tValidation Loss 1: 0.057682 \ttime: 172min\t\n",
      "Epoch: 39/50 \tTraining Loss: 0.058882 \tValidation Loss 1: 0.064210 \ttime: 177min\t\n",
      "Epoch: 40/50 \tTraining Loss: 0.059119 \tValidation Loss 1: 0.056436 \ttime: 181min\t\n",
      "Epoch: 41/50 \tTraining Loss: 0.058217 \tValidation Loss 1: 0.054616 \ttime: 185min\t\n",
      "Epoch: 42/50 \tTraining Loss: 0.057236 \tValidation Loss 1: 0.056522 \ttime: 190min\t\n",
      "Epoch: 43/50 \tTraining Loss: 0.057046 \tValidation Loss 1: 0.053022 \ttime: 194min\t\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "######### Weighted neural network  ##########  #### 8 modes\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Training loop :\n",
    "#######################\n",
    "\n",
    "# Initialisation of the loss value\n",
    "train_losses_save, valid_losses_save = [], []\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "# from carbontracker import parser\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    compteur = 0  # 计数器\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "        \n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "    model_3.train()\n",
    "    model_4.train()\n",
    "    model_5.train()\n",
    "    model_6.train()\n",
    "    model_7.train()\n",
    "    model_8.train()\n",
    "    model_W.train()\n",
    "        \n",
    "    for data, target in train_loader:\n",
    "\n",
    "        data,target=data.to(device,dtype=torch.float64),target.to(device,dtype=torch.float64)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_1.zero_grad()\n",
    "        optimizer_2.zero_grad()\n",
    "        optimizer_3.zero_grad()\n",
    "        optimizer_4.zero_grad()\n",
    "        optimizer_5.zero_grad()\n",
    "        optimizer_6.zero_grad()\n",
    "        optimizer_7.zero_grad()\n",
    "        optimizer_8.zero_grad()\n",
    "        optimizer_W.zero_grad()\n",
    "            \n",
    "        ################################################################\n",
    "        # Calculation of the final output as a combination of the 8 sub-models\n",
    "        ################################################################\n",
    "            \n",
    "        output_W = model_W(data.double())  # [bs, 1, 178, 358]\n",
    "        output_1 = model_1(data.double()) \n",
    "        output_2 = model_2(data.double()) \n",
    "        output_3 = model_3(data.double()) \n",
    "        output_4 = model_4(data.double()) \n",
    "        output_5 = model_5(data.double()) \n",
    "        output_6 = model_6(data.double()) \n",
    "        output_7 = model_7(data.double()) \n",
    "        output_8 = model_8(data.double()) \n",
    "            \n",
    "        # Weighted average\n",
    "        \n",
    "        W1 = output_W[:,0,:,:]\n",
    "        W1 = torch.reshape(W1, (batch_size, 1,178, 358))\n",
    "        W2 = output_W[:,1,:,:]\n",
    "        W2 = torch.reshape(W2, (batch_size, 1,178, 358))\n",
    "        W3 = output_W[:,2,:,:]\n",
    "        W3 = torch.reshape(W3, (batch_size, 1,178, 358))\n",
    "        W4 = output_W[:,3,:,:]\n",
    "        W4 = torch.reshape(W4, (batch_size, 1,178, 358))\n",
    "        W5 = output_W[:,4,:,:]\n",
    "        W5 = torch.reshape(W5, (batch_size, 1,178, 358))\n",
    "        W6 = output_W[:,5,:,:]\n",
    "        W6 = torch.reshape(W6, (batch_size, 1,178, 358))\n",
    "        W7 = output_W[:,6,:,:]\n",
    "        W7 = torch.reshape(W7, (batch_size, 1,178, 358))\n",
    "        W8 = output_W[:,7,:,:]\n",
    "        W8 = torch.reshape(W8, (batch_size, 1,178, 358))\n",
    "        \n",
    "        output_1 = torch.mul(output_1, W1)  # [bs, 1, 178, 358]\n",
    "        output_2 = torch.mul(output_2, W2)\n",
    "        output_3 = torch.mul(output_3, W3)\n",
    "        output_4 = torch.mul(output_4, W4)\n",
    "        output_5 = torch.mul(output_5, W5)\n",
    "        output_6 = torch.mul(output_6, W6)\n",
    "        output_7 = torch.mul(output_7, W7)\n",
    "        output_8 = torch.mul(output_8, W8)\n",
    "            \n",
    "        # Concatenation\n",
    "        concat = torch.cat((output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8), 1) \n",
    "            \n",
    "        # Sum on the last dimension\n",
    "        output = torch.sum(concat, 1)  # [bs, 178, 358]\n",
    "        output = torch.reshape(output, (batch_size, 1,178, 358)) # Reshape to apply the continental mask\n",
    "            \n",
    "        ######################################################\n",
    "        # Loss computation\n",
    "        ######################################################\n",
    "        \n",
    "        compteur = compteur + 1\n",
    "        \n",
    "        output2 = output\n",
    "        target2 = target\n",
    "        \n",
    "        where_are_NaNs = np.isnan(target.cpu())  #  NaN 值的位置为 True，其他位置为 False\n",
    "        \n",
    "        #  Training only on 50N - 50S(排除极区数据，仅在赤道附近)\n",
    "        \n",
    "        where_are_NaNs[:,0,0:39,:] = 1\n",
    "        where_are_NaNs[:,0,139:178,:] = 1\n",
    "        \n",
    "        where_are_NaNs = where_are_NaNs.bool()\n",
    "        where_are_NaNs = ~where_are_NaNs\n",
    "        where_are_NaNs = where_are_NaNs.to(device)\n",
    "        \n",
    "        output2 = torch.masked_select(output2, where_are_NaNs)\n",
    "        target2 = torch.masked_select(target2, where_are_NaNs)\n",
    "                 \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output2.double(), target2.double())\n",
    "\n",
    "        ######################################################\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        ######################################################\n",
    "        \n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_1.step()\n",
    "        optimizer_2.step()\n",
    "        optimizer_3.step()\n",
    "        optimizer_4.step()\n",
    "        optimizer_5.step()\n",
    "        optimizer_6.step()\n",
    "        optimizer_7.step()\n",
    "        optimizer_8.step()\n",
    "        optimizer_W.step()\n",
    "        \n",
    "        # updatse training los\n",
    "        train_loss += loss.item()\n",
    "           \n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "    scheduler3.step()\n",
    "    scheduler4.step()\n",
    "    scheduler5.step()\n",
    "    scheduler6.step()\n",
    "    scheduler7.step()\n",
    "    scheduler8.step()\n",
    "    schedulerW.step()\n",
    "    \n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    model_3.eval()\n",
    "    model_4.eval()\n",
    "    model_5.eval()\n",
    "    model_6.eval()\n",
    "    model_7.eval()\n",
    "    model_8.eval()\n",
    "    model_W.eval()\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "\n",
    "        data,target=data.to(device,dtype=torch.float),target.to(device,dtype=torch.float)\n",
    "            \n",
    "        output_W = model_W(data.double()) \n",
    "        output_1 = model_1(data.double()) \n",
    "        output_2 = model_2(data.double()) \n",
    "        output_3 = model_3(data.double()) \n",
    "        output_4 = model_4(data.double())\n",
    "        output_5 = model_5(data.double()) \n",
    "        output_6 = model_6(data.double()) \n",
    "        output_7 = model_7(data.double())\n",
    "        output_8 = model_8(data.double())\n",
    "            \n",
    "        \n",
    "        W1 = output_W[:,0,:,:]\n",
    "        W1 = torch.reshape(W1, (batch_size, 1,178, 358))\n",
    "        W2 = output_W[:,1,:,:]\n",
    "        W2 = torch.reshape(W2, (batch_size, 1,178, 358))\n",
    "        W3 = output_W[:,2,:,:]\n",
    "        W3 = torch.reshape(W3, (batch_size, 1,178, 358))\n",
    "        W4 = output_W[:,3,:,:]\n",
    "        W4 = torch.reshape(W4, (batch_size, 1,178, 358))\n",
    "        W5 = output_W[:,4,:,:]\n",
    "        W5 = torch.reshape(W5, (batch_size, 1,178, 358))\n",
    "        W6 = output_W[:,5,:,:]\n",
    "        W6 = torch.reshape(W6, (batch_size, 1,178, 358))\n",
    "        W7 = output_W[:,6,:,:]\n",
    "        W7 = torch.reshape(W7, (batch_size, 1,178, 358))\n",
    "        W8 = output_W[:,7,:,:]\n",
    "        W8 = torch.reshape(W8, (batch_size, 1,178, 358))\n",
    "        \n",
    "        output_1 = torch.mul(output_1, W1)\n",
    "        output_2 = torch.mul(output_2, W2)\n",
    "        output_3 = torch.mul(output_3, W3)\n",
    "        output_4 = torch.mul(output_4, W4)\n",
    "        output_5 = torch.mul(output_5, W5)\n",
    "        output_6 = torch.mul(output_6, W6)\n",
    "        output_7 = torch.mul(output_7, W7)\n",
    "        output_8 = torch.mul(output_8, W8)\n",
    "\n",
    "        concat = torch.cat((output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8), 1)   \n",
    "        output = torch.sum(concat, 1)\n",
    "        output = torch.reshape(output, (batch_size, 1,178, 358)) \n",
    "          \n",
    "        output2 = output\n",
    "        target2 = target\n",
    "        \n",
    "        where_are_NaNs = np.isnan(target.cpu())\n",
    "        \n",
    "        where_are_NaNs[:,0,0:39,:] = 1\n",
    "        where_are_NaNs[:,0,139:178,:] = 1\n",
    "        \n",
    "        where_are_NaNs = where_are_NaNs.bool()\n",
    "        where_are_NaNs = ~where_are_NaNs\n",
    "        where_are_NaNs = where_are_NaNs.to(device)\n",
    "        \n",
    "        output2 = torch.masked_select(output2, where_are_NaNs)\n",
    "        target2 = torch.masked_select(target2, where_are_NaNs)\n",
    "        \n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output2.double(), target2.double())\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)  # 每个epoch\n",
    "    valid_loss = valid_loss/len(val_loader.sampler)\n",
    "    \n",
    "    train_losses_save.append(train_loss)\n",
    "    valid_losses_save.append(valid_loss)\n",
    "    \n",
    "    # Save the loss in files\n",
    "    np.save(folder_name2 + 'train_losses_save.npy', train_losses_save)\n",
    "    np.save(folder_name2 + 'valid_losses_save.npy', valid_losses_save)\n",
    "    \n",
    "    # print loss/training curves to monitor overfitting / convergence\n",
    "    if loss_fig ==1 and epoch % loss_fig_epoque_affichage ==0 :\n",
    "        fig1=plt.figure(figsize=(3,3))\n",
    "        plt.plot(valid_losses_save, label='Validation loss',color='orange')\n",
    "        plt.plot(train_losses_save, label='Training loss',color='blue', linestyle='dashed')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    tps_t = time.process_time() \n",
    "    epoch_time = round((tps_t - tps_ini)/60)\n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss 1: {:.6f} \\ttime: {}min\\t'.format(epoch, n_epochs, train_loss, valid_loss, epoch_time))      \n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        torch.save(model_1.state_dict(), model_save_1 )   \n",
    "        torch.save(model_2.state_dict(), model_save_2 ) \n",
    "        torch.save(model_3.state_dict(), model_save_3 ) \n",
    "        torch.save(model_4.state_dict(), model_save_4 ) \n",
    "        torch.save(model_5.state_dict(), model_save_5 ) \n",
    "        torch.save(model_6.state_dict(), model_save_6 ) \n",
    "        torch.save(model_7.state_dict(), model_save_7 ) \n",
    "        torch.save(model_8.state_dict(), model_save_8 ) \n",
    "        torch.save(model_W.state_dict(), model_save_W ) \n",
    "        \n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    \n",
    "tps_final = time.process_time() \n",
    "tps_final_min = round((tps_final - tps_ini)/60,2)\n",
    "print('Time computation : {} min'.format(tps_final_min))\n",
    "\n",
    "########################################################################\n",
    "### Check overfitting / save loss\n",
    "########################################################################\n",
    "\n",
    "plt.plot(valid_losses_save, label='Validation loss',color='orange')\n",
    "plt.plot(train_losses_save, label='Training loss',color='blue', linestyle='dashed')\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig('./result/' + folder_name2 + 'Loss', dpi= 100,bbox_inches = \"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
