{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "import netCDF4 as nc\n",
    "\n",
    "\n",
    "class STDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 region_name = 'Gulf',\n",
    "                 folder_path='../data/',\n",
    "                 reference_file = '/home/data2/pengguohang/My_Ocean/challenge/oisst_monthly_201001-201904.nc', \n",
    "                 label_path = '../',\n",
    "                 lat_min = 23,\n",
    "                 lat_max = 50, \n",
    "                 lon_min = -80,\n",
    "                 lon_max = -30,\n",
    "                 challenge = 'ST',\n",
    "                 add_time = False,\n",
    "                 ):\n",
    "        '''\n",
    "        Args:\n",
    "            region_name(str) : 提取的数据范围(Gulf )\n",
    "            folder_path(str) : 存放所有数据的文件夹 , \"/home/data2/pengguohang/My_Ocean/challenge\"\n",
    "            reference_file(str): 数据处理时的参考文件(参考mask 分辨率等)\n",
    "            lat_min, lat_max(int) : 纬度范围\n",
    "            lon_min, lon_max(int) : 经度范围\n",
    "            key(str) : SS(so), ST(st)\n",
    "        Returns:\n",
    "            input, label, lat, lon, depth\n",
    "        shape:\n",
    "            (var, month, lat, lon), (depth, month, lat, lon), (x, y, p), (x, y, 2), (36)\n",
    "\n",
    "        数据处理步骤：\n",
    "        1. 读取reference file，得到参考经纬度大小和参考mask\n",
    "        2. 根据参考经纬度对其他输入进行插值，统一大小\n",
    "        3. 0.25*0.25下采样到0.5*0.5\n",
    "        4. 提取子区域数据\n",
    "        5. 掩码处理。通过参考mask给予统一掩码\n",
    "        6. 计算气候学平均值，得到计算异常值\n",
    "        7. 最大最小归一化\n",
    "        8. 合并时间到input中，通过cos sin得到jd1 jd2\n",
    "        9. 将lat lon合并到input\n",
    "        10. 将数据中的nan全换为0\n",
    "        11. 将数据划分为train和test，分别保存\n",
    "\n",
    "        '''\n",
    "        self.lat_min = lat_min\n",
    "        self.lat_max = lat_max\n",
    "        self.lon_min = lon_min\n",
    "        self.lon_max = lon_max\n",
    "\n",
    "        if challenge == 'ST':\n",
    "            key = 'to'\n",
    "        elif challenge == 'SS':\n",
    "            key = 'so'\n",
    "\n",
    "        # 提取数据\n",
    "        self.input, self.mask = self.get_input_data(folder_path, reference_file)\n",
    "        self.label, self.lat, self.lon, self.depth= self.get_armor(label_path, key)\n",
    "        \n",
    "        self.input = torch.from_numpy(self.input.values).permute(1,0,2,3)\n",
    "        self.label = torch.from_numpy(self.label.values).permute(1,0,2,3)\n",
    "        self.lat = torch.from_numpy(self.lat.values)\n",
    "        self.lon = torch.from_numpy(self.lon.values)\n",
    "        self.depth = torch.from_numpy(self.depth.values)\n",
    "\n",
    "        # 将lat和lon合并到input中\n",
    "        time = self.input.shape[0]\n",
    "        lat = self.input.shape[2]\n",
    "        lon = self.input.shape[3]\n",
    "        expand_lat = self.lat.unsqueeze(0).unsqueeze(-1).repeat(time, 1, 1, lon)\n",
    "        expand_lon = self.lon.unsqueeze(0).unsqueeze(0).repeat(time, 1, lat, 1)\n",
    "        self.input = torch.cat((self.input, expand_lat, expand_lon), dim=1)\n",
    "\n",
    "        # 将时间合并到input中\n",
    "        if add_time:\n",
    "            ds = xr.open_dataset(reference_file)\n",
    "            time = ds.variables['time'][0:109].values  # 201001 - 201901\n",
    "            jd1 = torch.cos( torch.tensor(2*np.pi*(time/12)+1) )\n",
    "            jd2 = torch.sin( torch.tensor(2*np.pi*(time/12)+1) )\n",
    "            jd1 = jd1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, lat, lon)\n",
    "            jd2 = jd2.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, lat, lon)\n",
    "            # print('jd1, jd2', jd1.shape, jd2.shape)\n",
    "            self.input = torch.cat((self.input, jd1), dim=1)\n",
    "            self.input = torch.cat((self.input, jd2), dim=1)\n",
    "\n",
    "        # LSTM数据：增加seq_len维度\n",
    "        #  torch.Size([109, 12, 108, 200]) torch.Size([109, 36, 108, 200])\n",
    "        \n",
    "        # 将数据中的nan全换为0\n",
    "        self.input = torch.where(torch.isnan(self.input), torch.full_like(self.input, 0), self.input)\n",
    "        self.label = torch.where(torch.isnan(self.label), torch.full_like(self.label, 0), self.label)\n",
    "\n",
    "        # 总数据：0:109, 代表201001-201901\n",
    "        # 数据划分为train:201001-201806 test:201807-201901\n",
    "        # 从train中随机拿出10个月份的数据作为验证集\n",
    "        self.train_input = self.input[0:99, ...]\n",
    "        self.train_label = self.label[0:99, ...]\n",
    "        self.test_input = self.input[99:, ...]\n",
    "        self.test_label = self.label[99:, ...]\n",
    "        \n",
    "        print('train data: ', self.train_input.shape, self.train_label.shape)\n",
    "        print('test data: ', self.test_input.shape, self.test_label.shape)\n",
    "\n",
    "    def save_traindata(self, train_path):\n",
    "\n",
    "        # 1. 保存训练数据\n",
    "        time, var, lat, lon = self.train_input.shape\n",
    "        depth = self.train_label.shape[1]\n",
    "        # 创建一个新的 NetCDF 文件\n",
    "        dataset = nc.Dataset(train_path, 'w', format='NETCDF4')\n",
    "        # 创建维度\n",
    "        dataset.createDimension('time', time)  # Unlimited dimension\n",
    "        dataset.createDimension('var', var)\n",
    "        dataset.createDimension('depth', depth)\n",
    "        dataset.createDimension('lat', lat)\n",
    "        dataset.createDimension('lon', lon)\n",
    "        # 创建变量\n",
    "        input = dataset.createVariable('input', 'f4', ('time', 'var', 'lat', 'lon',))\n",
    "        label = dataset.createVariable('label', 'f4', ('time', 'depth', 'lat', 'lon',))\n",
    "        # 添加属性\n",
    "        dataset.description = 'This is a sample NetCDF file'\n",
    "        input.units = 'arbitrary_units'  # Adjust units if applicable\n",
    "        input.description = 'Training input data'\n",
    "        label.units = 'arbitrary_units'  # Adjust units if applicable\n",
    "        label.description = 'Training label data'\n",
    "        # 写入数据\n",
    "        input[:] = self.train_input\n",
    "        label[:] = self.train_label\n",
    "        # 关闭文件\n",
    "        dataset.close()\n",
    "        print(f'Data saved to {train_path}')\n",
    "\n",
    "    def save_testdata(self, test_path):\n",
    "\n",
    "        # 1. 保存训练数据\n",
    "        time, var, lat, lon = self.test_input.shape\n",
    "        depth = self.test_label.shape[1]\n",
    "        # 创建一个新的 NetCDF 文件\n",
    "        dataset = nc.Dataset(test_path, 'w', format='NETCDF4')\n",
    "        # 创建维度\n",
    "        dataset.createDimension('time', time)  # Unlimited dimension\n",
    "        dataset.createDimension('var', var)\n",
    "        dataset.createDimension('depth', depth)\n",
    "        dataset.createDimension('lat', lat)\n",
    "        dataset.createDimension('lon', lon)\n",
    "        # 创建变量\n",
    "        input = dataset.createVariable('input', 'f4', ('time', 'var', 'lat', 'lon',))\n",
    "        label = dataset.createVariable('label', 'f4', ('time', 'depth', 'lat', 'lon',))\n",
    "        # 添加属性\n",
    "        dataset.description = 'This is a sample NetCDF file'\n",
    "        input.units = 'arbitrary_units'  # Adjust units if applicable\n",
    "        input.description = 'Training input data'\n",
    "        label.units = 'arbitrary_units'  # Adjust units if applicable\n",
    "        label.description = 'Training label data'\n",
    "        # 写入数据\n",
    "        input[:] = self.test_input\n",
    "        label[:] = self.test_label\n",
    "        # 关闭文件\n",
    "        dataset.close()\n",
    "        print(f'Data saved to {test_path}')\n",
    "\n",
    "\n",
    "    def get_sub(self, data, latitude, longitude):\n",
    "        \"\"\"\n",
    "        提取子区域的数据\n",
    "\n",
    "        input:\n",
    "        lat_min, lat_max, lon_min, lon_max: 子区域范围\n",
    "        data: 原始数据\n",
    "        latitude, longitude: 经纬度数据\n",
    "\n",
    "        return: subset_data, subset_lat, subset_lon\n",
    "        \"\"\"\n",
    "        # 找到对应的索引\n",
    "        lat_indices = np.where((latitude >= self.lat_min) & (latitude <= self.lat_max))[0]\n",
    "        lon_indices = np.where((longitude >= self.lon_min) & (longitude <= self.lon_max))[0]\n",
    "        # 提取子集数据\n",
    "        subset_data = data[:, lat_indices, :][:, :, lon_indices]\n",
    "        # 提取相应的经纬度数组\n",
    "        subset_lat = latitude[lat_indices]\n",
    "        subset_lon = longitude[lon_indices]\n",
    "\n",
    "        return subset_data, subset_lat, subset_lon\n",
    "\n",
    "\n",
    "    def compute_climatological_mean_and_anomalies(self, data):\n",
    "        \"\"\"\n",
    "        计算每个变量的气候学平均值, 从而计算其异常值\n",
    "        input: data (xarray.Dataset or xarray.DataArray): 包含多个变量的时间序列数据，维度为 (time, lat, lon)。\n",
    "        return: xarray.Dataset or xarray.DataArray: 包含异常值的数据集，维度为 (time, lat, lon)。\n",
    "        \"\"\"\n",
    "        # 时间维度名为 'time'\n",
    "        # print(\"Dimensions of data:\", data.dims)\n",
    "        \n",
    "        # 计算气候学平均值（沿着time维度求平均）\n",
    "        clim_mean = data.mean(dim='time')\n",
    "        \n",
    "        # 扩展气候学平均值，使其具有与原始数据相同的 time 维度\n",
    "        clim_mean_expanded = clim_mean.broadcast_like(data)\n",
    "        \n",
    "        # 从原始数据中减去气候学平均值得到异常值\n",
    "        anomalies = data - clim_mean_expanded\n",
    "        \n",
    "        return anomalies\n",
    "\n",
    "\n",
    "    def min_max(self, data):\n",
    "        \"\"\"\n",
    "        对输入数据按变量进行归一化\n",
    "\n",
    "        input:(var, time, lat, lon)\n",
    "        output: (var, time, lat, lon)\n",
    "        \"\"\"\n",
    "        minmax = []\n",
    "        for i in range(data.shape[0]):\n",
    "            var_data = data[i]\n",
    "            var_min = var_data.min(dim='time')\n",
    "            var_max = var_data.max(dim='time')\n",
    "            normalized_var_data = (var_data - var_min) / (var_max - var_min)\n",
    "            minmax.append(normalized_var_data)\n",
    "            # normalized_data.loc[dict(var=var)] = normalized_var_data\n",
    "\n",
    "        minmax = xr.concat(minmax, dim='file')\n",
    "        return minmax\n",
    "\n",
    "\n",
    "    def get_input_data(self, folder_path, reference_file):\n",
    "        \"\"\"\n",
    "        提取输入数据并裁剪\n",
    "        folder_path, reference_file: 数据文件夹地址 及 参考数据文件地址\n",
    "        \n",
    "        return:  (var, time, lat, lon)\n",
    "        \"\"\"\n",
    "        # 1、提取文件名\n",
    "        nc_files = [file for file in os.listdir(folder_path) if file.endswith('.nc')]\n",
    "        # 存储数据\n",
    "        data_all = []\n",
    "\n",
    "        # 2、先加载reference data, 作为网格插值的基准\n",
    "        ref_ds = xr.open_dataset(reference_file)\n",
    "        ref_lat = ref_ds['lat']\n",
    "        ref_lon = ref_ds['lon']\n",
    "        ref_data = ref_ds['data'][0:109, ...]   # torch.Size([109, 108, 200])\n",
    "        # 0.25*0.25下采样到0.5*0.5\n",
    "        data, sub_ref_lat, sub_ref_lon = self.down_sample(ref_data, ref_lat, ref_lon)\n",
    "        sub_ref_data = xr.DataArray(data.squeeze(0), dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": sub_ref_lat, \"lon\": sub_ref_lon})\n",
    "        # 提取子区域\n",
    "        ref_subset_data, ref_subset_lat, ref_subset_lon = self.get_sub(sub_ref_data, sub_ref_lat, sub_ref_lon)\n",
    "        # print('ref sub: ', ref_subset_data.shape, ref_data.shape)\n",
    "        # 将 -999.0 的值转换为 np.nan\n",
    "        mask = np.where(ref_subset_data == -999.0, np.nan, ref_subset_data)\n",
    "        ref_subset_data = xr.DataArray(mask, dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": ref_subset_lat, \"lon\": ref_subset_lon})\n",
    "        data_all.append(ref_subset_data)\n",
    "\n",
    "        # 3、逐个加载.nc文件并进行插值\n",
    "        for file in nc_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            # 提取前109个时间步的数据\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            data = ds['data'][:109, ...]  \n",
    "            # 将 'data' 插值到目标经纬度网格\n",
    "            interpolated_data = data.interp(lat=ref_lat, lon=ref_lon)\n",
    "            # 0.25*0.25下采样到0.5*0.5\n",
    "            data, lat, lon = self.down_sample(interpolated_data, ref_lat, ref_lon)\n",
    "            data = xr.DataArray(data.squeeze(0), dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": lat, \"lon\": lon})\n",
    "            # print('after sample: ', data.shape, lat.shape, lon.shape)\n",
    "            # 提取子区域\n",
    "            subset_data, subset_lat, subset_lon = self.get_sub(data, lat, lon)\n",
    "            # print('sub_set: ', subset_data.shape, subset_lat.shape, subset_lon.shape)\n",
    "            # 掩码处理：通过reference的nan值将所有数据相同位置的数字换为nan\n",
    "            nan_mask = np.isnan(ref_subset_data)\n",
    "            # print('mask: ', nan_mask.shape)\n",
    "            # print(nan_mask)\n",
    "            masked_data = np.where(nan_mask, np.nan, subset_data)\n",
    "            masked_data = xr.DataArray(masked_data, dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": ref_subset_lat, \"lon\": ref_subset_lon})\n",
    "            \n",
    "            data_all.append(masked_data)\n",
    "\n",
    "\n",
    "        # 将所有插值后的数据堆叠在一起\n",
    "        data_all = xr.concat(data_all, dim='file')\n",
    "        # 将数据中绝对值大于100的数值替换为NaN\n",
    "        data_all = data_all.where(np.abs(data_all) <= 100, np.nan)\n",
    "        # 计算数据异常值 - 减去 climatological mean\n",
    "        data_all = self.compute_climatological_mean_and_anomalies(data_all)\n",
    "        # 最大最小归一化\n",
    "        data_all = self.min_max(data_all)\n",
    "\n",
    "        # print('shape of region:', data_all.shape)\n",
    "        return data_all, nan_mask  # mask: (109, 54, 100)\n",
    "\n",
    "\n",
    "    def down_sample(self, data, lat_list, lon_list):\n",
    "        '''\n",
    "        0.25*0.25下采样到0.5*0.5\n",
    "\n",
    "        in: (t, lat, lon)\n",
    "        out: DataArray dim=(t, lat, lon)\n",
    "        '''\n",
    "        data = torch.tensor(data.values)  # array --> tensor\n",
    "        if data.dim() == 3:\n",
    "            data = data.unsqueeze(0)\n",
    "\n",
    "        lat, lon = data.shape[-2], data.shape[-1]\n",
    "        new_lat, new_lon = int(lat / 2), int(lon / 2)  # 目标尺寸\n",
    "        new_size = (new_lat, new_lon)\n",
    "\n",
    "        data = F.interpolate(data, size=new_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return data, lat_list[::2], lon_list[::2]\n",
    "    \n",
    "    \n",
    "    def get_armor(self, path, key):\n",
    "        '''\n",
    "        提取label\n",
    "\n",
    "        armor数据如下:\n",
    "        depth (36,)\n",
    "        latitude (688,)\n",
    "        longitude (1439,)\n",
    "        time (313,)\n",
    "        mlotst (313, 688, 1439)\n",
    "        so (313, 36, 688, 1439)\n",
    "        to (313, 36, 688, 1439)\n",
    "        \n",
    "        return: (depth, time, lat, lon)\n",
    "        '''\n",
    "        f = xr.open_dataset(path, chunks={'time': 1})\n",
    "        data = f[key][204:313, ...]\n",
    "        depth = f['depth']\n",
    "        lat = f['latitude']\n",
    "        lon = f['longitude']\n",
    "\n",
    "        # down_sample\n",
    "        sub_data, lat, lon = self.down_sample(data, lat, lon)\n",
    "        print('in armor: ', sub_data.shape, lat.shape, lon[:719].shape)\n",
    "        # data = xr.DataArray(sub_data, dims=[\"time\", \"depth\", \"lat\", \"lon\"], coords={\"lat\": lat, \"lon\": lon[:719]})\n",
    "        data = xr.DataArray(\n",
    "    sub_data,\n",
    "    dims=[\"time\", \"depth\", \"latitude\", \"longitude\"],\n",
    "    coords={\"latitude\": lat, \"longitude\": lon[:719]}\n",
    ")\n",
    "        print('in armor: ', data.shape, lat.shape, lon.shape)\n",
    "        # print('begin:', data.shape)\n",
    "        # 找到对应的索引\n",
    "        lat_indices = np.where((lat >= self.lat_min) & (lat <= self.lat_max))[0]\n",
    "        lon_indices = np.where((lon >= self.lon_min) & (lon <= self.lon_max))[0]\n",
    "        # print('lat,lon:', lat_indices.shape, lon_indices.shape)\n",
    "\n",
    "        # 提取子集数据\n",
    "        subset_data = data[:, :, lat_indices, lon_indices].transpose('depth', 'time',  'latitude', 'longitude')\n",
    "        # print('end:', subset_data.shape)\n",
    "\n",
    "        # 提取相应的经纬度数组\n",
    "        subset_lat = lat[lat_indices]\n",
    "        subset_lon = lon[lon_indices]\n",
    "\n",
    "        # 计算数据异常值 - 减去 climatological mean\n",
    "        # print(subset_data.dims)\n",
    "        subset_data = self.compute_climatological_mean_and_anomalies(subset_data)\n",
    "\n",
    "        # minmax归一化\n",
    "        subset_data = self.min_max(subset_data)\n",
    "\n",
    "        # print('return:', subset_data.shape)\n",
    "\n",
    "        return subset_data, subset_lat, subset_lon, depth\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.input[idx]   # (var, lat, lon) or (seq, var, lat, lon)\n",
    "        label = self.label[idx]    # (dept, lat, lon) or (seq, dept, lat, lon)\n",
    "        lat = self.lat\n",
    "        lon = self.lon\n",
    "        depth = self.depth\n",
    "        mask = torch.tensor(self.mask[0, 0, ...].values)  # (1, t, lat ,lon)  ->  (lat, lon)\n",
    "        \n",
    "        return inputs.float() , label.float() , mask, lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "194400 / 54 / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存SS-Gulf数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/vwnd_monthly_201001-201904.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/sss_cci_monthly_201001_201912_data.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/swh_monthly_201001_201912_data.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/oisst_monthly_201001-201904.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/uwnd_monthly_201001-201904.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/sla_monthly_201001_201901.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/adt_monthly_201001-201912.nc\n",
      "in armor:  torch.Size([109, 36, 344, 719]) (344,) (719,)\n",
      "in armor:  (109, 36, 344, 719) (344,) (720,)\n",
      "train data:  torch.Size([99, 12, 54, 100]) torch.Size([99, 36, 54, 100])\n",
      "test data:  torch.Size([10, 12, 54, 100]) torch.Size([10, 36, 54, 100])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config_file = '../config/ST_Gulf_FNN.yaml'\n",
    "with open(config_file, 'r') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "dataset = args['dataset']\n",
    "train_dataset = STDataset(\n",
    "                 region_name = dataset['region_name'],\n",
    "                 folder_path= dataset['folder_path'],\n",
    "                 reference_file = dataset['reference_file'], \n",
    "                 label_path = dataset['label_path'],\n",
    "                 lat_min = dataset['lat_min'],\n",
    "                 lat_max = dataset['lat_max'], \n",
    "                 lon_min = dataset['lon_min'],\n",
    "                 lon_max = dataset['lon_max'],\n",
    "                 challenge = args['challenge_name'],\n",
    "                 add_time = True, \n",
    "                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/data2/pengguohang/My_Ocean/challenge/data/train.nc\n",
      "Data saved to /home/data2/pengguohang/My_Ocean/challenge/data/test.nc\n"
     ]
    }
   ],
   "source": [
    "train_path = '/home/data2/pengguohang/My_Ocean/challenge/data/train.nc'\n",
    "test_path = '/home/data2/pengguohang/My_Ocean/challenge/data/test.nc'\n",
    "\n",
    "train_dataset.save_traindata(train_path)\n",
    "train_dataset.save_testdata(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
