{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 数据归一化问题\n",
    "# 输入数据处理：基于reference_file插值\n",
    "\n",
    "class STDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 region_name = 'Gulf',\n",
    "                 folder_path='../data/',\n",
    "                 reference_file = '/home/data2/pengguohang/My_Ocean/challenge/oisst_monthly_201001-201904.nc', \n",
    "                 label_path = '../',\n",
    "                 lat_min = 23,\n",
    "                 lat_max = 50, \n",
    "                 lon_min = -80,\n",
    "                 lon_max = -30,\n",
    "                 challenge = 'ST',\n",
    "                 add_time = False,\n",
    "                 if_train = True,\n",
    "                 seq_len = 0,\n",
    "                 get_201809 = False,\n",
    "                 ):\n",
    "        '''\n",
    "        提取201001-201901的每月数据\n",
    "        Args:\n",
    "            region_name(str) : 提取的数据范围(Gulf )\n",
    "            folder_path(str) : 存放所有数据的文件夹 , \"/home/data2/pengguohang/My_Ocean/challenge\"\n",
    "            reference_file(str): 数据处理时的参考文件(参考mask 分辨率等)\n",
    "            lat_min, lat_max(int) : 纬度范围\n",
    "            lon_min, lon_max(int) : 经度范围\n",
    "            key(str) : SS(so), ST(st)\n",
    "        Returns:\n",
    "            input, label, lat, lon, depth\n",
    "        shape:\n",
    "            (var, month, lat, lon), (depth, month, lat, lon), (x, y, p), (x, y, 2), (36)\n",
    "\n",
    "            lat_min, lat_max, lon_min, lon_max, data, latitude, longitude\n",
    "        '''\n",
    "        self.lat_min = lat_min\n",
    "        self.lat_max = lat_max\n",
    "        self.lon_min = lon_min\n",
    "        self.lon_max = lon_max\n",
    "\n",
    "        if challenge == 'ST':\n",
    "            key = 'to'\n",
    "        elif challenge == 'SS':\n",
    "            key = 'so'\n",
    "\n",
    "        # 提取数据\n",
    "        self.input = self.get_input_data(folder_path, reference_file)\n",
    "        self.label, self.lat, self.lon, self.depth, self.mask = self.get_armor(label_path, key)\n",
    "        self.mask = torch.where(self.mask, 0, 1)  # 将True False 换为0 1，false代表非nan值处\n",
    "        # mask: (36, 109, 54, 100)\n",
    "        \n",
    "        self.input = torch.from_numpy(self.input.values).permute(1,0,2,3)\n",
    "        self.label = torch.from_numpy(self.label.values).permute(1,0,2,3)\n",
    "        self.lat = torch.from_numpy(self.lat.values)\n",
    "        self.lon = torch.from_numpy(self.lon.values)\n",
    "        self.depth = torch.from_numpy(self.depth.values)\n",
    "\n",
    "        # 将lat和lon合并到input中\n",
    "        time = self.input.shape[0]\n",
    "        lat = self.input.shape[2]\n",
    "        lon = self.input.shape[3]\n",
    "        expand_lat = self.lat.unsqueeze(0).unsqueeze(-1).repeat(time, 1, 1, lon)\n",
    "        expand_lon = self.lon.unsqueeze(0).unsqueeze(0).repeat(time, 1, lat, 1)\n",
    "        self.input = torch.cat((self.input, expand_lat, expand_lon), dim=1)\n",
    "\n",
    "        # 将时间合并到input中\n",
    "        if add_time:\n",
    "            ds = xr.open_dataset(reference_file)\n",
    "            time = ds.variables['time'][0:109].values  # 201001 - 201901\n",
    "            jd1 = torch.cos( torch.tensor(2*np.pi*(time/12)+1) )\n",
    "            jd2 = torch.sin( torch.tensor(2*np.pi*(time/12)+1) )\n",
    "            jd1 = jd1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, lat, lon)\n",
    "            jd2 = jd2.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, lat, lon)\n",
    "            # print('jd1, jd2', jd1.shape, jd2.shape)\n",
    "            self.input = torch.cat((self.input, jd1), dim=1)\n",
    "            self.input = torch.cat((self.input, jd2), dim=1)\n",
    "\n",
    "        # LSTM数据：增加seq_len维度\n",
    "        test_len = 10  # 从train中随机拿出10个月份的数据作为验证集\n",
    "        if seq_len > 0:\n",
    "            num = self.input.shape[0] / seq_len\n",
    "            var = self.input.shape[1]\n",
    "            depth = self.label.shape[1]\n",
    "\n",
    "            self.input = self.input[0:int(num)*seq_len, ...].reshape(int(num), seq_len, var, lat, lon)\n",
    "            self.label = self.label[0:int(num)*seq_len, ...].reshape(int(num), seq_len, depth, lat, lon)\n",
    "\n",
    "            test_len = int(np.ceil(10 / seq_len))\n",
    "        #  torch.Size([109, 12, 108, 200]) torch.Size([109, 36, 108, 200])\n",
    "        \n",
    "        # 将数据中的nan全换为0\n",
    "        self.input = torch.where(torch.isnan(self.input), torch.full_like(self.input, 0), self.input)\n",
    "        self.label = torch.where(torch.isnan(self.label), torch.full_like(self.label, 0), self.label)\n",
    "\n",
    "        # 总数据：0:109, 代表201001-201901\n",
    "        # 数据划分为train:201001-201803 test:201804-201901\n",
    "        # 从train中随机拿出10个月份的数据作为验证集\n",
    "        train_len = self.input.shape[0] - test_len\n",
    "        # print(train_len, test_len)\n",
    "        if if_train:\n",
    "            self.input = self.input[0:train_len, ...]\n",
    "            self.label = self.label[0:train_len, ...]\n",
    "        else:\n",
    "            self.input = self.input[train_len:, ...]\n",
    "            self.label = self.label[train_len:, ...]\n",
    "        \n",
    "\n",
    "        \n",
    "        print('shape of variable: ', self.input.shape, self.label.shape, self.lat.shape, self.lon.shape, self.depth.shape)\n",
    "\n",
    "    def get_sub(self, data, latitude, longitude):\n",
    "        \"\"\"\n",
    "        提取子区域的数据\n",
    "\n",
    "        input:\n",
    "        lat_min, lat_max, lon_min, lon_max: 子区域范围\n",
    "        data: 原始数据\n",
    "        latitude, longitude: 经纬度数据\n",
    "\n",
    "        return: subset_data, subset_lat, subset_lon\n",
    "        \"\"\"\n",
    "        # 找到对应的索引\n",
    "        lat_indices = np.where((latitude >= self.lat_min) & (latitude <= self.lat_max))[0]\n",
    "        lon_indices = np.where((longitude >= self.lon_min) & (longitude <= self.lon_max))[0]\n",
    "        # 提取子集数据\n",
    "        subset_data = data[:, lat_indices, :][:, :, lon_indices]\n",
    "        # 提取相应的经纬度数组\n",
    "        subset_lat = latitude[lat_indices]\n",
    "        subset_lon = longitude[lon_indices]\n",
    "\n",
    "        return subset_data, subset_lat, subset_lon\n",
    "\n",
    "\n",
    "    def compute_climatological_mean_and_anomalies(self, data):\n",
    "        \"\"\"\n",
    "        计算每个变量的气候学平均值, 从而计算其异常值\n",
    "        input: data (xarray.Dataset or xarray.DataArray): 包含多个变量的时间序列数据，维度为 (time, lat, lon)。\n",
    "        return: xarray.Dataset or xarray.DataArray: 包含异常值的数据集，维度为 (time, lat, lon)。\n",
    "        \"\"\"\n",
    "        # 时间维度名为 'time'\n",
    "        # print(\"Dimensions of data:\", data.dims)\n",
    "        \n",
    "        # 计算气候学平均值（沿着time维度求平均）\n",
    "        clim_mean = data.mean(dim='time')\n",
    "        \n",
    "        # 扩展气候学平均值，使其具有与原始数据相同的 time 维度\n",
    "        clim_mean_expanded = clim_mean.broadcast_like(data)\n",
    "        \n",
    "        # 从原始数据中减去气候学平均值得到异常值\n",
    "        anomalies = data - clim_mean_expanded\n",
    "        \n",
    "        return anomalies\n",
    "\n",
    "\n",
    "    def min_max(self, data):\n",
    "        \"\"\"\n",
    "        对输入数据按变量进行归一化\n",
    "\n",
    "        input:(var, time, lat, lon)\n",
    "        output: (var, time, lat, lon)\n",
    "        \"\"\"\n",
    "        minmax = []\n",
    "        for i in range(data.shape[0]):\n",
    "            var_data = data[i]\n",
    "            var_min = var_data.min(dim='time')\n",
    "            var_max = var_data.max(dim='time')\n",
    "            normalized_var_data = (var_data - var_min) / (var_max - var_min)\n",
    "            minmax.append(normalized_var_data)\n",
    "            # normalized_data.loc[dict(var=var)] = normalized_var_data\n",
    "\n",
    "        minmax = xr.concat(minmax, dim='file')\n",
    "        return minmax\n",
    "\n",
    "\n",
    "    def get_input_data(self, folder_path, reference_file):\n",
    "        \"\"\"\n",
    "        提取输入数据并裁剪\n",
    "        folder_path, reference_file: 数据文件夹地址 及 参考数据文件地址\n",
    "        \n",
    "        return:  (var, time, lat, lon)\n",
    "        \"\"\"\n",
    "        # 1、提取文件名\n",
    "        nc_files = [file for file in os.listdir(folder_path) if file.endswith('.nc')]\n",
    "        # 存储数据\n",
    "        data_all = []\n",
    "\n",
    "        # 2、先加载reference data, 作为网格插值的基准\n",
    "        ref_ds = xr.open_dataset(reference_file)\n",
    "        ref_lat = ref_ds['lat']\n",
    "        ref_lon = ref_ds['lon']\n",
    "        ref_data = ref_ds['data'][0:109, ...]   # torch.Size([109, 108, 200])\n",
    "        # 0.25*0.25下采样到0.5*0.5\n",
    "        data, sub_ref_lat, sub_ref_lon = self.down_sample(ref_data, ref_lat, ref_lon)\n",
    "        sub_ref_data = xr.DataArray(data.squeeze(0), dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": sub_ref_lat, \"lon\": sub_ref_lon})\n",
    "        # 提取子区域\n",
    "        ref_subset_data, ref_subset_lat, ref_subset_lon = self.get_sub(sub_ref_data, sub_ref_lat, sub_ref_lon)\n",
    "        # print('ref sub: ', ref_subset_data.shape, ref_data.shape)\n",
    "        # 将 -999.0 的值转换为 np.nan\n",
    "        mask = np.where(ref_subset_data == -999.0, np.nan, ref_subset_data)\n",
    "        ref_subset_data = xr.DataArray(mask, dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": ref_subset_lat, \"lon\": ref_subset_lon})\n",
    "        data_all.append(ref_subset_data)\n",
    "\n",
    "        # 3、逐个加载.nc文件并进行插值\n",
    "        for file in nc_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            # 提取前109个时间步的数据\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            data = ds['data'][:109, ...]  \n",
    "            # 将 'data' 插值到目标经纬度网格\n",
    "            interpolated_data = data.interp(lat=ref_lat, lon=ref_lon)\n",
    "            # 0.25*0.25下采样到0.5*0.5\n",
    "            data, lat, lon = self.down_sample(interpolated_data, ref_lat, ref_lon)\n",
    "            data = xr.DataArray(data.squeeze(0), dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": lat, \"lon\": lon})\n",
    "            # print('after sample: ', data.shape, lat.shape, lon.shape)\n",
    "            # 提取子区域\n",
    "            subset_data, subset_lat, subset_lon = self.get_sub(data, lat, lon)\n",
    "            # print('sub_set: ', subset_data.shape, subset_lat.shape, subset_lon.shape)\n",
    "            # 掩码处理：通过reference的nan值将所有数据相同位置的数字换为nan\n",
    "            nan_mask = np.isnan(ref_subset_data)\n",
    "            # print('mask: ', nan_mask.shape)\n",
    "            # print(nan_mask)\n",
    "            masked_data = np.where(nan_mask, np.nan, subset_data)\n",
    "            masked_data = xr.DataArray(masked_data, dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": ref_subset_lat, \"lon\": ref_subset_lon})\n",
    "            \n",
    "            data_all.append(masked_data)\n",
    "\n",
    "\n",
    "        # 将所有插值后的数据堆叠在一起\n",
    "        data_all = xr.concat(data_all, dim='file')\n",
    "        # 将数据中绝对值大于100的数值替换为NaN\n",
    "        data_all = data_all.where(np.abs(data_all) <= 100, np.nan)\n",
    "        # 计算数据异常值 - 减去 climatological mean\n",
    "        data_all = self.compute_climatological_mean_and_anomalies(data_all)\n",
    "        # 最大最小归一化\n",
    "        data_all = self.min_max(data_all)\n",
    "\n",
    "        # print('shape of region:', data_all.shape)\n",
    "        return data_all\n",
    "\n",
    "\n",
    "    def down_sample(self, data, lat_list, lon_list):\n",
    "        '''\n",
    "        0.25*0.25下采样到0.5*0.5\n",
    "\n",
    "        in: (t, lat, lon)\n",
    "        out: DataArray dim=(t, lat, lon)\n",
    "        '''\n",
    "        data = torch.tensor(data.values)  # array --> tensor\n",
    "        if data.dim() == 3:\n",
    "            data = data.unsqueeze(0)\n",
    "\n",
    "        lat, lon = data.shape[-2], data.shape[-1]\n",
    "        new_lat, new_lon = int(lat / 2), int(lon / 2)  # 目标尺寸\n",
    "        new_size = (new_lat, new_lon)\n",
    "\n",
    "        data = F.interpolate(data, size=new_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return data, lat_list[::2], lon_list[::2]\n",
    "    \n",
    "    \n",
    "    def get_armor(self, path, key):\n",
    "        '''\n",
    "        提取label\n",
    "\n",
    "        armor数据如下:\n",
    "        depth (36,)\n",
    "        latitude (688,)\n",
    "        longitude (1439,)\n",
    "        time (313,)\n",
    "        mlotst (313, 688, 1439)\n",
    "        so (313, 36, 688, 1439)\n",
    "        to (313, 36, 688, 1439)\n",
    "        \n",
    "        return: (depth, time, lat, lon)\n",
    "        '''\n",
    "        f = xr.open_dataset(path, chunks={'time': 1})\n",
    "        data = f[key][204:313, ...]\n",
    "        depth = f['depth']\n",
    "        lat = f['latitude']\n",
    "        lon = f['longitude']\n",
    "\n",
    "        # down_sample\n",
    "        sub_data, lat, lon = self.down_sample(data, lat, lon)\n",
    "        data = xr.DataArray(sub_data, dims=[\"time\", \"depth\", \"latitude\", \"longitude\"], coords={\"latitude\": lat, \"longitude\": lon[:719]})\n",
    "\n",
    "        # 找到对应的索引\n",
    "        lat_indices = np.where((lat >= self.lat_min) & (lat <= self.lat_max))[0]\n",
    "        lon_indices = np.where((lon >= self.lon_min) & (lon <= self.lon_max))[0]\n",
    "        # print('lat,lon:', lat_indices.shape, lon_indices.shape)\n",
    "\n",
    "        # 提取子集数据\n",
    "        subset_data = data[:, :, lat_indices, lon_indices].transpose('depth', 'time',  'latitude', 'longitude')\n",
    "        # print('end:', subset_data.shape)\n",
    "\n",
    "        # 提取相应的经纬度数组\n",
    "        subset_lat = lat[lat_indices]\n",
    "        subset_lon = lon[lon_indices]\n",
    "\n",
    "        # 计算数据异常值 - 减去 climatological mean\n",
    "        # print(subset_data.dims)\n",
    "        subset_data = self.compute_climatological_mean_and_anomalies(subset_data)\n",
    "        nan_mask = np.isnan(subset_data)\n",
    "        nan_mask = torch.tensor(nan_mask.values)\n",
    "        # print('armor: ', subset_data)\n",
    "\n",
    "        # minmax归一化\n",
    "        subset_data = self.min_max(subset_data)\n",
    "\n",
    "        # print('return:', subset_data.shape)\n",
    "\n",
    "        return subset_data, subset_lat, subset_lon, depth, nan_mask\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.input[idx]   # (var, lat, lon) or (seq, var, lat, lon)\n",
    "        label = self.label[idx]    # (dept, lat, lon) or (seq, dept, lat, lon)\n",
    "        lat = self.lat\n",
    "        lon = self.lon\n",
    "        depth = self.depth\n",
    "        mask = self.mask[0, 0, ...]  # (bs, t, lat ,lon)  ->  (lat, lon)\n",
    "        \n",
    "        return inputs.float() , label.float() , mask, lat, lon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of variables: torch.Size([283, 5, 10, 54, 100]) torch.Size([283, 1, 15, 54, 100]) torch.Size([54]) torch.Size([100]) torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "from ST import STDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = STDataset()\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pengguohang/python_test/Ocean/OceanBench2/dataset/queck_test.ipynb 单元格 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.18.166.69/home/pengguohang/python_test/Ocean/OceanBench2/dataset/queck_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(loader))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.18.166.69/home/pengguohang/python_test/Ocean/OceanBench2/dataset/queck_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.18.166.69/home/pengguohang/python_test/Ocean/OceanBench2/dataset/queck_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data = next(iter(loader))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
