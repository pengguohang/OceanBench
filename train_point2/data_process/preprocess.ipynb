{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理201001-201901的每月数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub(data, latitude, longitude, lat_min, lat_max, lon_min, lon_max):\n",
    "        \"\"\"\n",
    "        提取子区域的数据\n",
    "\n",
    "        input:\n",
    "        lat_min, lat_max, lon_min, lon_max: 子区域范围\n",
    "        data: 原始数据\n",
    "        latitude, longitude: 经纬度数据\n",
    "\n",
    "        return: subset_data, subset_lat, subset_lon\n",
    "        \"\"\"\n",
    "        \n",
    "        lat_indices = np.where((latitude >= lat_min) & (latitude <= lat_max))[0]  # 找到对应的索引\n",
    "        lon_indices = np.where((longitude >= lon_min) & (longitude <= lon_max))[0]\n",
    "        \n",
    "        subset_data = data[:, lat_indices, :][:, :, lon_indices]  # 提取子集数据\n",
    "        subset_lat = latitude[lat_indices]  # 提取相应的经纬度数组\n",
    "        subset_lon = longitude[lon_indices]\n",
    "\n",
    "        return subset_data, subset_lat, subset_lon\n",
    "\n",
    "def down_sample(data, lat_list, lon_list):\n",
    "        '''\n",
    "        0.25*0.25下采样到0.5*0.5\n",
    "\n",
    "        in: (t, lat, lon)\n",
    "        out: DataArray dim=(t, lat, lon)\n",
    "        '''\n",
    "        data = torch.tensor(data.values)  # array --> tensor\n",
    "        if data.dim() == 3:\n",
    "            data = data.unsqueeze(0)\n",
    "\n",
    "        lat, lon = data.shape[-2], data.shape[-1]\n",
    "        new_lat, new_lon = int(lat / 2), int(lon / 2)  # 目标尺寸\n",
    "        new_size = (new_lat, new_lon)\n",
    "        data = F.interpolate(data, size=new_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return data, lat_list[::2], lon_list[::2]\n",
    "\n",
    "def compute_climatological_mean_and_anomalies(data):\n",
    "        \"\"\"\n",
    "        计算每个变量的气候学平均值, 从而计算其异常值\n",
    "        input: data (xarray.Dataset or xarray.DataArray): 包含多个变量的时间序列数据，维度为 (time, lat, lon)。\n",
    "        return: xarray.Dataset or xarray.DataArray: 包含异常值的数据集，维度为 (time, lat, lon)。\n",
    "        \"\"\"\n",
    "        # 时间维度名为 'time'\n",
    "        # print(\"Dimensions of data:\", data.dims)\n",
    "        clim_mean = data.mean(dim='time')  # 计算气候学平均值（沿着time维度求平均）\n",
    "        clim_mean_expanded = clim_mean.broadcast_like(data)\n",
    "        anomalies = data - clim_mean_expanded  # 从原始数据中减去气候学平均值得到异常值\n",
    "        \n",
    "        return anomalies\n",
    "\n",
    "\n",
    "def min_max(data):\n",
    "        \"\"\"\n",
    "        对输入数据按变量进行归一化\n",
    "\n",
    "        input:(var, time, lat, lon)\n",
    "        output: (var, time, lat, lon)\n",
    "        \"\"\"\n",
    "        minmax = []\n",
    "        for i in range(data.shape[0]):\n",
    "                var_data = data[i]\n",
    "                var_min = var_data.min(dim='time')\n",
    "                var_max = var_data.max(dim='time')\n",
    "                normalized_var_data = (var_data - var_min) / (var_max - var_min)\n",
    "                minmax.append(normalized_var_data)\n",
    "                # normalized_data.loc[dict(var=var)] = normalized_var_data\n",
    "\n",
    "        minmax = xr.concat(minmax, dim='file')\n",
    "        return minmax\n",
    "\n",
    "def get_input_data(folder_path, reference_file, lat_min, lat_max, lon_min, lon_max):\n",
    "        \"\"\"\n",
    "        提取输入数据并裁剪\n",
    "        folder_path, reference_file: 数据文件夹地址 及 参考数据文件地址\n",
    "        \n",
    "        return:  (var, time, lat, lon)\n",
    "        \"\"\"\n",
    "        # 1、提取文件名\n",
    "        nc_files = [file for file in os.listdir(folder_path) if file.endswith('.nc')]\n",
    "        data_all = []  # 存储所有数据\n",
    "\n",
    "        # 2、先加载reference data, 作为网格插值的基准\n",
    "        ref_ds = xr.open_dataset(reference_file)\n",
    "        ref_lat = ref_ds['lat']\n",
    "        ref_lon = ref_ds['lon']\n",
    "        ref_data = ref_ds['data'][0:109, ...]   # torch.Size([109, 108, 200])\n",
    "        data, sub_ref_lat, sub_ref_lon = down_sample(ref_data, ref_lat, ref_lon)  # 0.25*0.25下采样到0.5*0.5\n",
    "        sub_ref_data = xr.DataArray(data.squeeze(0), dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": sub_ref_lat, \"lon\": sub_ref_lon})\n",
    "        # 提取子区域\n",
    "        ref_subset_data, ref_subset_lat, ref_subset_lon = get_sub(sub_ref_data, sub_ref_lat, sub_ref_lon, lat_min, lat_max, lon_min, lon_max)\n",
    "        # print('ref sub: ', ref_subset_data.shape, ref_data.shape)\n",
    "        # 将 -999.0 的值转换为 np.nan\n",
    "        mask = np.where(ref_subset_data == -999.0, np.nan, ref_subset_data)\n",
    "        ref_subset_data = xr.DataArray(mask, dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": ref_subset_lat, \"lon\": ref_subset_lon})\n",
    "        data_all.append(ref_subset_data)\n",
    "\n",
    "        # 3、逐个加载.nc文件并进行插值\n",
    "        for file in nc_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            data = ds['data'][:109, ...]  # 提取前109个时间步的数据 \n",
    "            interpolated_data = data.interp(lat=ref_lat, lon=ref_lon)  # 将 'data' 插值到目标经纬度网格\n",
    "            data, lat, lon = down_sample(interpolated_data, ref_lat, ref_lon)  # 0.25*0.25下采样到0.5*0.5\n",
    "            data = xr.DataArray(data.squeeze(0), dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": lat, \"lon\": lon})\n",
    "            # print('after sample: ', data.shape, lat.shape, lon.shape)\n",
    "            subset_data, subset_lat, subset_lon = get_sub(data, lat, lon, lat_min, lat_max, lon_min, lon_max)  # 提取子区域\n",
    "            # print('sub_set: ', subset_data.shape, subset_lat.shape, subset_lon.shape)\n",
    "            # 掩码处理：通过reference的nan值将所有数据相同位置的数字换为nan\n",
    "            nan_mask = np.isnan(ref_subset_data)\n",
    "            # print('mask: ', nan_mask.shape)\n",
    "            # print(nan_mask)\n",
    "            masked_data = np.where(nan_mask, np.nan, subset_data)\n",
    "            masked_data = xr.DataArray(masked_data, dims=[\"time\", \"lat\", \"lon\"], coords={\"lat\": ref_subset_lat, \"lon\": ref_subset_lon})\n",
    "            \n",
    "            data_all.append(masked_data)\n",
    "\n",
    "        data_all = xr.concat(data_all, dim='file')  # 将所有插值后的数据堆叠在一起\n",
    "        data_all = data_all.where(np.abs(data_all) <= 100, np.nan)  # 将数据中绝对值大于100的数值替换为NaN\n",
    "        data_all = compute_climatological_mean_and_anomalies(data_all)  # 计算数据异常值 - 减去 climatological mean\n",
    "        data_all = min_max(data_all)  # 最大最小归一化\n",
    "\n",
    "        # print('shape of region:', data_all.shape)\n",
    "        return data_all\n",
    "\n",
    "def get_armor(path, key, lat_min, lat_max, lon_min, lon_max):\n",
    "        '''\n",
    "        提取label\n",
    "\n",
    "        armor数据如下:\n",
    "        depth (36,)\n",
    "        latitude (688,)\n",
    "        longitude (1439,)\n",
    "        time (313,)\n",
    "        mlotst (313, 688, 1439)\n",
    "        so (313, 36, 688, 1439)\n",
    "        to (313, 36, 688, 1439)\n",
    "        \n",
    "        return: (depth, time, lat, lon)\n",
    "        '''\n",
    "        f = xr.open_dataset(path, chunks={'time': 1})\n",
    "        data = f[key][204:313, ...]\n",
    "        depth = f['depth']\n",
    "        lat = f['latitude']\n",
    "        lon = f['longitude']\n",
    "\n",
    "        # down_sample\n",
    "        sub_data, lat, lon = down_sample(data, lat, lon)\n",
    "        data = xr.DataArray(sub_data, dims=[\"time\", \"depth\", \"latitude\", \"longitude\"], coords={\"latitude\": lat, \"longitude\": lon[:719]})\n",
    "\n",
    "        # 找到对应的索引\n",
    "        lat_indices = np.where((lat >= lat_min) & (lat <= lat_max))[0]\n",
    "        lon_indices = np.where((lon >= lon_min) & (lon <= lon_max))[0]\n",
    "        # print('lat,lon:', lat_indices.shape, lon_indices.shape)\n",
    "\n",
    "        # 提取子集数据\n",
    "        subset_data = data[:, :, lat_indices, lon_indices].transpose('depth', 'time',  'latitude', 'longitude')\n",
    "        # print('end:', subset_data.shape)\n",
    "\n",
    "        # 提取相应的经纬度数组\n",
    "        subset_lat = lat[lat_indices]\n",
    "        subset_lon = lon[lon_indices]\n",
    "\n",
    "        # 计算数据异常值 - 减去 climatological mean\n",
    "        # print(subset_data.dims)\n",
    "        subset_data = compute_climatological_mean_and_anomalies(subset_data)\n",
    "        nan_mask = np.isnan(subset_data)\n",
    "        nan_mask = torch.tensor(nan_mask.values)\n",
    "        # print('armor: ', subset_data)\n",
    "\n",
    "        # minmax归一化\n",
    "        subset_data = min_max(subset_data)\n",
    "\n",
    "        # print('return:', subset_data.shape)\n",
    "\n",
    "        return subset_data, subset_lat, subset_lon, depth, nan_mask\n",
    "\n",
    "def precess_data(input, label, lat, lon, depth, mask, reference_file):\n",
    "        # mask: (36, 109, 54, 100)\n",
    "        mask = torch.where(mask, 0, 1)  # 将True False 换为0 1，false代表非nan值处\n",
    "        input = torch.from_numpy(input.values).permute(1,0,2,3)\n",
    "        label = torch.from_numpy(label.values).permute(1,0,2,3)\n",
    "        lat = torch.from_numpy(lat.values)\n",
    "        lon = torch.from_numpy(lon.values)\n",
    "        depth = torch.from_numpy(depth.values)\n",
    "\n",
    "        # 将lat和lon合并到input中\n",
    "        time = input.shape[0]\n",
    "        lat = input.shape[2]\n",
    "        lon = input.shape[3]\n",
    "        expand_lat = lat.unsqueeze(0).unsqueeze(-1).repeat(time, 1, 1, lon)\n",
    "        expand_lon = lon.unsqueeze(0).unsqueeze(0).repeat(time, 1, lat, 1)\n",
    "        input = torch.cat((input, expand_lat, expand_lon), dim=1)\n",
    "\n",
    "        # 将时间合并到input中\n",
    "        ds = xr.open_dataset(reference_file)\n",
    "        time = ds.variables['time'][0:109].values  # 201001 - 201901\n",
    "        jd1 = torch.cos( torch.tensor(2*np.pi*(time/12)+1) )\n",
    "        jd2 = torch.sin( torch.tensor(2*np.pi*(time/12)+1) )\n",
    "        jd1 = jd1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, lat, lon)\n",
    "        jd2 = jd2.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, lat, lon)\n",
    "        # print('jd1, jd2', jd1.shape, jd2.shape)\n",
    "        input = torch.cat((input, jd1), dim=1)\n",
    "        input = torch.cat((input, jd2), dim=1)\n",
    "        \n",
    "        # 将数据中的nan全换为0\n",
    "        input = torch.where(torch.isnan(input), torch.full_like(input, 0), input)\n",
    "        label = torch.where(torch.isnan(label), torch.full_like(label, 0), label)\n",
    "\n",
    "        # 总数据：0:109, 代表201001-201901\n",
    "        # 数据划分为train:201001-201803 test:201804-201901\n",
    "\n",
    "        \n",
    "        print('shape of variable: ', input.shape, label.shape, lat.shape, lon.shape, depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/vwnd_monthly_201001-201904.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/sss_cci_monthly_201001_201912_data.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/swh_monthly_201001_201912_data.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/oisst_monthly_201001-201904.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/uwnd_monthly_201001-201904.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/sla_monthly_201001_201901.nc\n",
      "Processing file: /home/data2/pengguohang/My_Ocean/challenge/adt_monthly_201001-201912.nc\n"
     ]
    }
   ],
   "source": [
    "reference_file = '/home/data2/pengguohang/My_Ocean/challenge/oisst_monthly_201001-201904.nc'\n",
    "folder_path='/home/data2/pengguohang/My_Ocean/challenge'\n",
    "label_path = '/home/data2/pengguohang/My_Ocean/CMEMS/armor_montly_1993_2019/armor_1993_2019.nc'\n",
    "\n",
    "# 墨西哥湾区域\n",
    "lat_min = 23\n",
    "lat_max = 50\n",
    "lon_min = -80   \n",
    "lon_max = -30\n",
    "\n",
    "key = 'to'  # 'so' or 'to'\n",
    "\n",
    "inputs = get_input_data(folder_path, reference_file, lat_min, lat_max, lon_min, lon_max)\n",
    "labels, lat, lon, depth, mask = get_armor(label_path, key, lat_min, lat_max, lon_min, lon_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([109, 8, 54, 100])\n",
      "torch.Size([109, 36, 54, 100])\n",
      "torch.Size([54, 100])\n"
     ]
    }
   ],
   "source": [
    "mask_data = torch.where(mask, 0, 1)  # 将True False 换为0 1，false代表非nan值处\n",
    "mask_data = mask_data[0, 0, :, :]\n",
    "input_data = torch.from_numpy(inputs.values).permute(1,0,2,3)\n",
    "label_data = torch.from_numpy(labels.values).permute(1,0,2,3)\n",
    "lat_data = torch.from_numpy(lat.values)\n",
    "lon_data = torch.from_numpy(lon.values)\n",
    "depth_data = torch.from_numpy(depth.values)\n",
    "print(input_data.shape)\n",
    "print(label_data.shape)\n",
    "print(mask_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import netCDF4 as nc\n",
    "\n",
    "# 创建一个NetCDF文件\n",
    "dataset = nc.Dataset('/home/data2/pengguohang/My_Ocean/challenge/data.nc', 'w', format='NETCDF4')\n",
    "\n",
    "# 创建维度\n",
    "dim1 = dataset.createDimension('vars', 8)\n",
    "dim2 = dataset.createDimension('month', 109)\n",
    "dim3 = dataset.createDimension('lat', 54)\n",
    "dim4 = dataset.createDimension('lon', 100)\n",
    "dim5 = dataset.createDimension('depth', 36)\n",
    "\n",
    "# 创建变量并关联到维度\n",
    "input = dataset.createVariable('input', 'f4', ('month', 'vars', 'lat', 'lon'))\n",
    "label = dataset.createVariable('label', 'f4', ('month', 'depth', 'lat', 'lon'))\n",
    "lat = dataset.createVariable('lat', 'f4', ('lat',))\n",
    "lon = dataset.createVariable('lon', 'f4', ('lon',))\n",
    "depth = dataset.createVariable('depth', 'f4', ('depth',))\n",
    "mask = dataset.createVariable('mask', 'f4', ('lat', 'lon'))\n",
    "\n",
    "# 将PyTorch张量的数据复制到NetCDF变量中\n",
    "input[:] = input_data.numpy()  # 需要将张量转换为NumPy数组\n",
    "label[:] = label_data.numpy()\n",
    "lat[:] = lat_data.numpy()\n",
    "lon[:] = lon_data.numpy()\n",
    "depth[:] = depth_data.numpy()\n",
    "mask[:] = mask_data.numpy()\n",
    "\n",
    "\n",
    "# 添加一些属性\n",
    "input.units = 'none'\n",
    "input.long_name = 'inputs'\n",
    "\n",
    "# 关闭文件\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'label', 'lat', 'lon', 'depth', 'mask'])\n",
      "input:  (109, 8, 54, 100)\n",
      "label:  (109, 36, 54, 100)\n",
      "lat:  (54,)\n",
      "lon:  (100,)\n",
      "depth:  (36,)\n",
      "mask:  (54, 100)\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '/home/data2/pengguohang/My_Ocean/challenge/data/data.nc'\n",
    "with nc.Dataset(path) as ds:\n",
    "    print(ds.variables.keys())\n",
    "    input = ds['input'][:]\n",
    "    print('input: ', input.shape)\n",
    "    label = ds['label'][:]\n",
    "    print('label: ', label.shape)\n",
    "    lat = ds['lat'][:]\n",
    "    print('lat: ', lat.shape)\n",
    "    lon = ds['lon'][:]\n",
    "    print('lon: ', lon.shape)\n",
    "    depth = ds['depth'][:]\n",
    "    print('depth: ', depth.shape)\n",
    "    mask = ds['mask'][:]\n",
    "    print('mask: ', mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看CORA数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (depth: 102, latitude: 1671, longitude: 720, time: 313)\n",
      "Coordinates:\n",
      "  * depth      (depth) float32 1.0 3.0 5.0 10.0 15.0 ... 940.0 960.0 980.0 1e+03\n",
      "  * latitude   (latitude) float64 -77.0 -76.9 -76.8 -76.7 ... 89.8 89.9 90.0\n",
      "  * longitude  (longitude) float64 -180.0 -179.5 -179.0 ... 178.5 179.0 179.5\n",
      "  * time       (time) datetime64[ns] 1993-01-01 1993-02-01 ... 2019-01-01\n",
      "Data variables:\n",
      "    PSAL       (time, depth, latitude, longitude) float32 ...\n",
      "Attributes: (12/22)\n",
      "    Conventions:               CF-1.4\n",
      "    analysis_name:             OA_CORA5.2_\n",
      "    comment:                   V8.0 reference climatology and analysis parame...\n",
      "    creation_date:             20230926T220219L\n",
      "    data_manager:              Tanguy Szekely\n",
      "    easternmost_longitude:     179.5\n",
      "    ...                        ...\n",
      "    southernmost_latitude:     -77.0105\n",
      "    start_date:                2022-12-15\n",
      "    stop_date:                 2022-12-15\n",
      "    title:                     Global Ocean - Coriolis Observation Re-Analysi...\n",
      "    westernmost_longitude:     -180\n",
      "    copernicusmarine_version:  1.2.2\n"
     ]
    }
   ],
   "source": [
    "path = '/home/data2/pengguohang/My_Ocean/CMEMS/CORA_1993_2019_P1M/CORA_199301_201901_PSAL_P1M.nc'\n",
    "\n",
    "ds = xr.open_dataset(path)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理CCMP风速数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "['CCMP_Wind_Analysis_199301_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199302_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199303_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199304_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199305_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199306_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199307_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199308_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199309_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199310_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199311_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199312_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199401_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199402_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199403_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199404_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199405_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199406_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199407_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199408_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199409_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199410_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199411_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199412_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199501_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199502_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199503_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199504_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199505_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199506_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199507_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199508_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199509_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199510_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199511_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199512_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199601_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199602_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199603_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199604_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199605_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199606_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199607_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199608_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199609_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199610_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199611_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199612_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199701_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199702_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199703_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199704_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199705_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199706_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199707_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199708_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199709_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199710_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199711_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199712_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199801_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199802_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199803_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199804_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199805_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199806_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199807_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199808_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199809_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199810_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199811_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199812_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199901_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199902_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199903_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199904_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199905_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199906_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199907_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199908_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199909_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199910_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199911_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_199912_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200001_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200002_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200003_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200004_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200005_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200006_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200007_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200008_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200009_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200010_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200011_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200012_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200101_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200102_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200103_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200104_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200105_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200106_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200107_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200108_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200109_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200110_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200111_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200112_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200202_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200203_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200204_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200205_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200206_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200207_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200208_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200209_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200210_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200211_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200212_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200301_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200302_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200303_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200304_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200305_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200306_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200307_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200308_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200309_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200310_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200311_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200312_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200401_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200402_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200403_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200404_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200405_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200406_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200407_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200408_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200409_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200410_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200411_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200412_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200501_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200502_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200503_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200504_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200505_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200506_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200507_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200508_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200509_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200510_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200511_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200512_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200601_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200602_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200603_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200604_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200605_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200606_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200607_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200608_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200609_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200610_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200611_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200612_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200701_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200702_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200703_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200704_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200705_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200706_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200707_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200708_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200709_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200710_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200711_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200712_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200801_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200802_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200803_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200804_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200805_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200806_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200807_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200808_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200809_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200810_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200811_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200812_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200901_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200902_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200903_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200904_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200905_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200906_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200907_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200908_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200909_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200910_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200911_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_200912_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201001_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201002_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201003_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201004_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201005_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201006_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201007_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201008_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201009_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201010_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201011_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201012_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201101_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201102_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201103_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201104_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201105_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201106_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201107_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201108_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201109_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201110_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201111_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201112_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201201_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201202_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201203_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201204_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201205_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201206_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201207_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201208_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201209_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201210_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201211_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201212_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201301_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201302_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201303_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201304_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201305_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201306_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201307_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201308_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201309_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201310_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201311_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201312_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201401_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201402_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201403_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201404_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201405_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201406_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201407_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201408_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201409_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201410_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201411_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201412_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201501_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201502_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201503_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201504_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201505_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201506_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201507_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201508_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201509_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201510_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201511_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201512_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201601_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201602_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201603_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201604_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201605_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201606_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201607_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201608_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201609_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201610_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201611_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201612_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201701_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201702_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201703_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201704_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201705_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201706_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201707_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201708_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201709_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201710_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201711_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201712_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201801_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201802_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201803_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201804_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201805_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201806_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201807_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201808_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201809_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201810_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201811_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201812_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201901_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201902_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201903_V02.0_L3.5_RSS.nc', 'CCMP_Wind_Analysis_201904_V02.0_L3.5_RSS.nc']\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/home/data2/pengguohang/My_Ocean/CCMP/ccmp_1993_201904_M'\n",
    "\n",
    "file_name = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.endswith('.nc') and f != 'CCMP_Wind_Analysis_climatology_V02.0_L3.5_RSS.nc']\n",
    "    )\n",
    "print(len(file_name))\n",
    "print(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
